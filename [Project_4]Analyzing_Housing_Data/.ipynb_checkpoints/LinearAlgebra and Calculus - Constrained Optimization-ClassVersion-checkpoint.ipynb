{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Linear Algebra and Calculus - Constrained Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%plot inline -w 600 -h 600 -r 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange multipliers\n",
    "\n",
    "Let's say we want to find the extreme points of a function $f(\\vec{x})$ under some constraints $g(\\vec{x})=0$, which means that we do not allow any extreme point, but only those that **also** satisfy the constraint.\n",
    "\n",
    "\n",
    "### Example 1\n",
    "\n",
    "Find the extreme points of $f(x,y)=5x^2+9y^2+4xy$ subject to $g(x,y)=x^2+y^2=1$ (which of course means that the points need to lie on a circle!).\n",
    "\n",
    "In two dimensions, $g(\\vec{x})=0$ describes a **contour** and $f(\\vec{x})$ describes a **surface**.\n",
    "\n",
    "Let's plot these - to better see what's happening, let's plot the function $f(\\vec{x})$ as **sets of contours**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% optimization function f\n",
    "f = @(x,y) 5*x.^2+9*y.^2+4*x.*y;\n",
    "\n",
    "% functional version of the constraint g to get circle\n",
    "gu = @(x) sqrt(1-x.^2);gd = @(x) -sqrt(1-x.^2);\n",
    "\n",
    "[x,y]=meshgrid([-2:.1:2]);\n",
    "\n",
    "contour(...); hold on\n",
    "...\n",
    "colorbar\n",
    "axis square\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to plot the contour maps of our example function $f(x,y)$ again, but this time centered a bit better to see how the contours interact with our constraint (which is also a contour):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% optimization function f\n",
    "f = @(x,y) 5*x.^2+9*y.^2+4*x.*y;\n",
    "\n",
    "% functional version of the constraint g to get circle\n",
    "gu = @(x) sqrt(1-x.^2);gd = @(x) -sqrt(1-x.^2);\n",
    "\n",
    "[x,y]=meshgrid([-2:.1:2]);\n",
    "\n",
    "% we will see why this is so later!\n",
    "secretSauce = f(-0.9239,0.3827);\n",
    "secretSauce2 = f(0.3827,0.9239);\n",
    "\n",
    "contour(x,y,f(x,y),[secretSauce-4:1:secretSauce2+4]); hold on\n",
    "contour(x,y,f(x,y),...,'LineWidth',2,'Color','k');\n",
    "contour(x,y,f(x,y),...,'LineWidth',2,'Color',[0.5 0.5 0.5]); \n",
    "\n",
    "...\n",
    "\n",
    "colorbar\n",
    "axis square\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we clearly see what is happening - as soon as the contours approach our constraint, the black and grey curves touch the contour - and at those points, clearly the contour's gradients are parallel! \n",
    "\n",
    "We also see that the inner black contour and the outer grey contour will be the functions minima and maxima satisfying the constraint!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrangian\n",
    "So, let's try to find the points that satisfy both properties at the same time by writing down some equations.\n",
    "\n",
    "For this, we have two conditions:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\vec{x}) = \\vec{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(\\vec{x})=0\n",
    "$$\n",
    "\n",
    "The key idea was to realize the following:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p>When $f_{opt}$ is a maximum or minimum of $f$ subject to the constraint, the contour line for $f(\\vec{x})=f_{opt}$ will be ... to the contour representing $g(\\vec{x})=0$.\n",
    "\n",
    "This in turn means that **at those extreme points** the two gradients of both functions $f$ and $g$ will be ... and we have: \n",
    "\n",
    "$$\n",
    "...\n",
    "$$\n",
    "</div>\n",
    "\n",
    "It is customary to rephrase our problem as finding the extreme points of a **new function** $L$, which is called the **Lagrangian**:\n",
    "\n",
    "$$\n",
    "L(\\vec{x},\\lambda):=...\n",
    "$$\n",
    "\n",
    "and we are looking for its extreme points, so:\n",
    "\n",
    "$$\n",
    "\\nabla L = \\left ( \\begin{matrix}\\frac{\\partial L}{\\partial \\vec{x}}\\\\\\frac{\\partial L}{\\partial \\lambda}\\end{matrix} \\right ) =\\vec{0}\n",
    "$$\n",
    "\n",
    "where we, of course, need to look for the derivatives to both types of parameters.\n",
    "\n",
    "Trivially, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\lambda} = 0 \\rightarrow g=0\n",
    "$$\n",
    "\n",
    "which simply reiterates our original constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 ctd\n",
    "So, let's do this approach for our function and constraint from above:\n",
    "\n",
    "$$\n",
    "L=f(x,y)+\\lambda g(x,y)=5x^2+9y^2+4xy+\\lambda(x^2+y^2-1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla L = \\left ( \\begin{matrix}\\frac{\\partial L}{\\partial \\vec{x}}\\\\\\frac{\\partial L}{\\partial \\lambda}\\end{matrix} \\right ) = \\left ( \\begin{matrix}10x+4y+\\lambda 2x\\\\18y+4x+\\lambda 2y\\\\x^2+y^2-1\\end{matrix} \\right ) =\\vec{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x=\\frac{-4y}{10+2\\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y(18+\\frac{-16}{10+2\\lambda}+2\\lambda)=0\\\\\n",
    "y(\\frac{18(10+2\\lambda)-16+2\\lambda(10+2\\lambda)}{10+2\\lambda})=0\\\\\n",
    "\\rightarrow(164+56\\lambda+4\\lambda^2)=0\\\\\n",
    "$$\n",
    "\n",
    "This will give us two lambdas, with which we substitute into the equation for $x$, which in turn gets substituted into the constraint equation $x^2+y^2-1$ and with that we can solve for **four** total points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% we get two lambdas\n",
    "lambdas = ...\n",
    "\n",
    "% some convenience factors (10+2*lambda)\n",
    "f1 = 10+2*lambdas(1);f2=10+2*lambdas(2);\n",
    "\n",
    "% we get four solutions for y\n",
    "y1 = roots([16/(f1^2)+1 0 -1])\n",
    "y2 = roots([16/(f2^2)+1 0 -1])\n",
    "\n",
    "% and four corresponding x-values\n",
    "x1 = [-4*y1(1)/f1; -4*y1(2)/f1]\n",
    "x2 = [-4*y2(1)/f2; -4*y2(2)/f2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have all points - let's plot them and the corresponding gradients of the two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "% optimization function f\n",
    "f = @(x,y) 5*x.^2+9*y.^2+4*x.*y;\n",
    "\n",
    "% functional version of the constraint g to get circle\n",
    "gu = @(x) sqrt(1-x.^2);gd = @(x) -sqrt(1-x.^2);\n",
    "\n",
    "[x,y]=meshgrid([-2:.1:2]);\n",
    "\n",
    "% now we know why this is so ^^\n",
    "secretSauce = f(-0.9239,0.3827);\n",
    "secretSauce2 = f(0.3827,0.9239);\n",
    "\n",
    "contour(x,y,f(x,y),[secretSauce-4:1:secretSauce2+4]); hold on\n",
    "contour(x,y,f(x,y),[secretSauce secretSauce],'LineWidth',2,'Color','k'); hold on\n",
    "contour(x,y,f(x,y),[secretSauce2 secretSauce2],'LineWidth',2,'Color',[0.5 0.5 0.5]); hold on\n",
    "\n",
    "fplot(gu,'r-','LineWidth',2);fplot(gd,'r-','LineWidth',2);\n",
    "\n",
    "% partial derivatives of f\n",
    "fx = @(x,y) ...;\n",
    "fy = @(x,y) ...;\n",
    "\n",
    "% partial derivatives of g\n",
    "gx = @(x,y) ...;\n",
    "gy = @(x,y) ...;\n",
    "\n",
    "xopt = [x1;x2];\n",
    "yopt = [y1;y2];\n",
    "for i=1:4\n",
    "    % plot the extreme point\n",
    "    plot(...,'o');\n",
    "    % plot the derivative at that point of f\n",
    "    drawArrow([xopt(i),xopt(i)+.06*fx(xopt(i),yopt(i))],...\n",
    "    [yopt(i),yopt(i)+.06*fy(xopt(i),yopt(i))],[-2 2],[-2 2],...\n",
    "    {'LineWidth',2});\n",
    "    % plot the derivative at that point of g\n",
    "    drawArrow([xopt(i),xopt(i)+.15*gx(xopt(i),yopt(i))],...\n",
    "    [yopt(i),yopt(i)+.15*gy(xopt(i),yopt(i))],[-2 2],[-2 2],...\n",
    "    {'LineWidth',2,'Color','r'});\n",
    "end\n",
    "\n",
    "\n",
    "colorbar\n",
    "axis square\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the extreme points are exactly where we want them to be and that the gradients of the constraint $g$ in red and those of the function $f$ in black are parallel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange multipliers applied to quadratic forms\n",
    "Let's try to derive this with Linear Algebra!\n",
    "\n",
    "Suppose we have a so-called quadratic form $q(\\vec{x})=\\vec{x}^{\\top}A\\vec{x}$ with a symmetric matrix $A$, for which we want to find its extreme points. \n",
    "\n",
    "In addition, however, we place a **constraint** on the vector $\\vec{x}$. We want it to be normalized to 1, so that $\\vec{x}^{\\top}\\vec{x}=1$. \n",
    "\n",
    "Now, we need to find a solution to  $\\vec{x}^{\\top}A\\vec{x}$ that also satisfies the constraint  $\\vec{x}^{\\top}\\vec{x}=1$.\n",
    "\n",
    "We obviously need to have:\n",
    "\n",
    "$$\n",
    "\\nabla\\vec{x}^{\\top}A\\vec{x}=0\n",
    "$$\n",
    "\n",
    "and the constraint becomes:\n",
    "\n",
    "$$\n",
    "\\rightarrow \\vec{x}^{\\top}\\vec{x}-1=0\n",
    "$$\n",
    "\n",
    "Given this constraint to be 0, our problem changes from finding a point where no perturbation in\n",
    "any direction changes $q(\\vec{x})$ - this would mean that we are looking for extreme points in general. Instead, we need to find a point at which **perturbations that satisfy the\n",
    "constraints do not change $q(\\vec{x})$**. So, with this we form our Lagrangian:\n",
    "\n",
    "$$\n",
    "L(\\vec{x},\\lambda)=\\vec{x}^{\\top}A\\vec{x}-\\lambda(\\vec{x}^{\\top}\\vec{x}-1)\n",
    "$$\n",
    "\n",
    "And we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\vec{x}}=2A\\vec{x}-\\lambda I\\vec{x}=0\\\\\n",
    "\\frac{\\partial L}{\\partial \\lambda}=\\vec{x}^{\\top}\\vec{x}-1=0\n",
    "$$\n",
    "\n",
    "From the first equation, we see that:\n",
    "\n",
    "$$\n",
    "(2A-\\lambda I)\\vec{x}=0\n",
    "$$\n",
    "\n",
    "which means that - other than the trivial solution $\\vec{x}=\\vec{0}$ - we get $A\\vec{x}=\\lambda\\vec{x}$, which is exactly a statement that the solutions will need to be **eigenvectors** of $A$!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example ctd with linear algebra\n",
    "Seeing that our example function is a quadratic function $f(x,y)=5x^2+9y^2+4xy$, we can identify the matrix $A$, which becomes:\n",
    "\n",
    "$$\n",
    "\\left (\\begin{matrix}\n",
    "... &...\\\\\n",
    "... &...\\\\\n",
    "\\end{matrix}\n",
    "\\right )\n",
    "$$\n",
    "\n",
    "and so we can solve and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [...];\n",
    "\n",
    "[evecs,evals]=eig(A)\n",
    "\n",
    "% now we know why this is so ^^\n",
    "secretSauce = f(-0.9239,0.3827);\n",
    "secretSauce2 = f(0.3827,0.9239);\n",
    "\n",
    "...\n",
    "\n",
    "% partial derivatives of f\n",
    "fx = @(x,y) ...;\n",
    "fy = @(x,y) ...;\n",
    "\n",
    "% partial derivatives of g\n",
    "gx = @(x,y) ...;\n",
    "gy = @(x,y) ...;\n",
    "\n",
    "\n",
    "for i=1:2\n",
    "    % print some stats\n",
    "    fprintf('quadratic form of eigenvector %d is x^T A x=%f, and x^T x=%f\\n',...\n",
    "    i,evecs(:,i)'*A*evecs(:,i),evecs(:,i)'*evecs(:,i));\n",
    "    % plot the extreme point\n",
    "    plot(evecs(1,i),evecs(2,i),'o');\n",
    "    % plot the derivative at that point of f\n",
    "    drawArrow([evecs(1,i),evecs(1,i)+.06*fx(evecs(1,i),evecs(2,i))],...\n",
    "    [evecs(2,i),evecs(2,i)+.06*fy(evecs(1,i),evecs(2,i))],[-2 2],[-2 2],...\n",
    "    {'LineWidth',2});\n",
    "    % plot the derivative at that point of g\n",
    "    drawArrow([evecs(1,i),evecs(1,i)+.15*gx(evecs(1,i),evecs(2,i))],...\n",
    "    [evecs(2,i),evecs(2,i)+.15*gy(evecs(1,i),evecs(2,i))],[-2 2],[-2 2],...\n",
    "    {'LineWidth',2,'Color','b'});\n",
    "end\n",
    "\n",
    "\n",
    "colorbar\n",
    "axis square\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the derivatives are parallel and that the chosen extreme points follow the contour behavior of $f$.\n",
    "\n",
    "Where are the other two points you may ask? They would appear if we had set $-\\lambda$ in the equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Find the extreme points of $x^2+y^2+8z^2+4xy-6yz+10xz$ subject to $x^2+y^2+z^2=1$ (which of course means that the points need to lie on a sphere!).\n",
    "\n",
    "The matrix $A$ becomes:\n",
    "\n",
    "$$\n",
    "\\left (\\begin{matrix}\n",
    "... &... &...\\\\\n",
    "... &... &...\\\\\n",
    "... &... &...\\\\\n",
    "\\end{matrix}\n",
    "\\right )\n",
    "$$\n",
    "\n",
    "and so we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [...];\n",
    "\n",
    "[evecs,evals]=eig(A)\n",
    "\n",
    "for i=1:3\n",
    "    fprintf('quadratic form of eigenvector %d is x^tAx=%f\\n',i,evecs(:,i)'*A*evecs(:,i));\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 - towards statistics!\n",
    "\n",
    "For a so-called multinomial distribution, we have an event $e$ with\n",
    "$K$ possible discrete, disjoint outcomes, where the probability of a certain outcome $k$ is given by:\n",
    "\n",
    "$$\n",
    "P(e = k) = p_k\n",
    "$$\n",
    "\n",
    "As a famous example, coin-flipping has a binomial distribution with $K = 2$, where $e = 1$ could mean that the coin shows \"heads\" and $e=2$ that it shows \"tail\".\n",
    "\n",
    "Let's say that we observe a total of $N$ events, then the likelihood of the end outcome is:\n",
    "\n",
    "$$\n",
    "\\Pi_{i=1}^{N}P(e_i|p)=\\Pi_{k}p_k^{N_k}\n",
    "$$\n",
    "\n",
    "with $N_k$ being the number of times we observed event $k$ in our sample. So, we want to maximize this likelihood, which is equivalent to **minimizing** the **log-likelihood** (yeay, logarithms for compression!):\n",
    "\n",
    "$$\n",
    "\\underset{p_k}{\\arg\\min} \\left ( -\\sum_k ... \\right )\n",
    "$$ \n",
    "\n",
    "subject to \n",
    "$$\n",
    "\\sum_k ... \\text{ and } p_k ..., \\forall k\n",
    "$$\n",
    "\n",
    "\n",
    "Let's form the Lagrangian from the two functions, keeping for now only the first constraint:\n",
    "\n",
    "$$\n",
    "L= ...\n",
    "$$\n",
    "\n",
    "and we get for the two derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p_k}=-N_k\\frac{1}{p_k}+\\lambda=0\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\lambda}=\\sum_k p_k-1=0\n",
    "$$\n",
    "\n",
    "How do we get rid of the pesky $p_k$ in the first equation? We multiply through with it and sum over all $k$ (think about why we can do this, even if $p_k$ can be 0!!):\n",
    "\n",
    "$$\n",
    "\\sum_k -N_k+\\lambda\\sum_k p_k=-N+\\lambda \\rightarrow \\lambda=N\n",
    "$$\n",
    "\n",
    "So our optimal $\\lambda$ is simply the number of total events. Let's substitute this back into the original equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p_k}=-N_k\\frac{1}{p_k}+N=0 \\rightarrow p_k=\\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "So that means that our maximum likelihood in a multinomial distribution occurs if all events are proportionally probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4 - Principal Components Analysis\n",
    "\n",
    "Let's go back to our linear data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = [[1 3.7];[2 5.4];[3 5.6];[3.2 7.0]; [4 7.6];[5 7.9];[6 9.5]];\n",
    "\n",
    "scatter(y(:,1),y(:,2)); grid on; xlim([0 7]); ylim([3 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this data lives in two dimensions, and we can also see that it looks like there is a lot of correlation in the data (which is equivalent to saying that they are linearly related)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ans =\n",
      "\n",
      "    0.9723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corr(y(:,1),y(:,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has two dimensions, but it seems that the direction that is most **interesting** is the one that points into the direction of the correlation.\n",
    "\n",
    "How can we find this? How do we find the direction in this data space that has the most correlation?\n",
    "\n",
    "Once we know this direction, we can then project all of our points onto this direction vector and **reduce the dimensionality of the problem**.\n",
    "\n",
    "This direction vector is known as the (first) **principal component $\\vec{w}_1$** of the data set $\\vec{y}$ and our process would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[1 3.7];[2 5.4];[3 5.6];[3.2 7.0]; [4 7.6];[5 7.9];[6 9.5]];\n",
    "scatter(y(:,1),y(:,2)); grid on; xlim([0 7]); ylim([3 10]); hold on\n",
    "\n",
    "% let's fit the line\n",
    "A=[...]; b = ...;\n",
    "coeff=A\\b;\n",
    "xplot = [0.5:0.5:6.5];\n",
    "plot(xplot,coeff(1)+coeff(2)*xplot,'linewidth',2)\n",
    "\n",
    "% let's project the points using our beautiful projection technique from above\n",
    "% for this, we need a vector of span 1 (it's a line!), which we get simply from\n",
    "% the gradient of our line equation!\n",
    "X = [1;coeff(2)];\n",
    "\n",
    "% here's the projection matrix\n",
    "P = ...;\n",
    "\n",
    "% now project all the points\n",
    "for p=1:size(y,1)\n",
    "    % do the projection - caution: the projection is only valid for a line through the origin!!!\n",
    "    yP = P*(...);\n",
    "    % plot the projected point\n",
    "    plot(yP(1),yP(2)+coeff(1),'b.');\n",
    "    % plot the projection line\n",
    "    plot([yP(1) y(p,1)],[yP(2)+coeff(1),y(p,2)],'k');\n",
    "end\n",
    "\n",
    "% plot the projection direction\n",
    "drawArrow([1.5,2],[coeff(1)+coeff(2)*1.5,coeff(1)+coeff(2)*2],[0 7],[3 10],{'LineWidth',2,'Color','b'});\n",
    "text(1.7,coeff(1)+coeff(2)*1.5,'$\\vec{w}_1$','Interpreter','latex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find this projection direction $\\vec{w}_1$ to n-dimensional datapoints $\\vec{y}$ in this linear case.\n",
    "\n",
    "So, for the projections $x_i$ of a datapoint $\\vec{y}_i$ onto $\\vec{w}_1$, we have simply the scalar product of $\\vec{y}_i$ with $\\vec{w}_1$, where we need to make sure that we subtracted the offset of the line first: \n",
    "\n",
    "$$\n",
    "x_i=\\vec{w}_1^{\\top}(\\vec{y}_i-\\vec{b})\n",
    "$$\n",
    "\n",
    "So, how do we put constraints on these projections? \n",
    "\n",
    "1. We would like all $x_i$ to have **maximum variance**, so that they capture most of the statistical information in the original data. This means that we will project the data into a basis which is based on directions of high variance (see the direction of $\\vec{w}_1$ above!).\n",
    "\n",
    "2. In addition, we require that the weight vector $\\vec{w}_1$ needs to be normalized, i.e., $\\vec{w}_1^{\\top}\\vec{w}_1=1$, since we are looking for a direction only.\n",
    "\n",
    "With this, our Lagrangian becomes:\n",
    "\n",
    "$$\n",
    "L(\\vec{w}_1,\\vec{b},\\lambda)=\\frac{1}{N}\\sum_i \\left ( x_i-\\frac{1}{N}\\sum_i x_i \\right )^2-\\lambda(\\vec{w}_1^{\\top}\\vec{w}_1-1)\\\\\n",
    "=\\frac{1}{N}\\sum_i \\left ( \\vec{w}_1^{\\top}(\\vec{y}_i-\\vec{b})-\\frac{1}{N}\\sum_i \\vec{w}_1^{\\top}(\\vec{y}_i-\\vec{b}) \\right )^2-\\lambda(\\vec{w}_1^{\\top}\\vec{w}_1-1)\\\\\n",
    "=\\frac{1}{N}\\sum_i \\left ( \\vec{w}_1^{\\top}(\\vec{y}_i-\\vec{b})-\\frac{1}{N}\\sum_i (\\vec{y}_i-\\vec{b})\\right )^2-\\lambda(\\vec{w}_1^{\\top}\\vec{w}_1-1)\n",
    "$$\n",
    "\n",
    "in the first bracket, the $b$ cancels (which makes sense since the direction doesn't depend on the offset!), so we get:\n",
    "\n",
    "$$\n",
    "=\\frac{1}{N}\\sum_i \\left ( \\vec{w}_1^{\\top}(\\vec{y}_i-\\vec{y}_{av})\\right )^2-\\lambda(\\vec{w}_1^{\\top}\\vec{w}_1-1)\\\\\n",
    "=\\frac{1}{N}\\vec{w}_1^{\\top}\\left ( \\sum_i (\\vec{y}_i-\\vec{y}_{av}) (\\vec{y}_i-\\vec{y}_{av})^{\\top} \\right ) \\vec{w}_1 -\\lambda(\\vec{w}_1^{\\top}\\vec{w}_1-1)\n",
    "$$\n",
    "\n",
    "And our first derivative becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\vec{w}_1}=2\\left ( \\frac{1}{N}\\sum_i (\\vec{y}_i-\\vec{y}_{av}) (\\vec{y}_i-\\vec{y}_{av})^{\\top} \\right ) \\vec{w}_1-2\\lambda\\vec{w}_1=0\\rightarrow \\left ( \\frac{1}{N}\\sum_i (\\vec{y}_i-\\vec{y}_{av}) (\\vec{y}_i-\\vec{y}_{av})^{\\top} \\right ) \\vec{w}_1 = \\lambda\\vec{w}_1\n",
    "$$\n",
    "\n",
    "And we realize the following:\n",
    "\n",
    "1. $$\n",
    "C=\\left ( \\frac{1}{N}\\sum_i (\\vec{y}_i-\\vec{y}_{av}) (\\vec{y}_i-\\vec{y}_{av})^{\\top} \\right )\n",
    "$$ is simply the covariance matrix.\n",
    "\n",
    "2. The equation above simply states that the optimal $\\vec{w}_1$ is an eigenvector of the covariance matrix $C$.\n",
    "\n",
    "Let's substitute $\\lambda$ back into the final equation of the Lagrangian to get:\n",
    "\n",
    "$$\n",
    "L=\\vec{w}_1^{\\top}\\lambda \\vec{w}_1 -\\lambda(\\vec{w}_1^{\\top}\\vec{w}_1-1)=\\lambda\n",
    "$$\n",
    "\n",
    "since we required that $\\vec{w}_1^{\\top} \\vec{w}_1=1$.\n",
    "\n",
    "Remember that our original constraint says to **maximize** the variance and with that $L$, so the $\\lambda$ we should choose for this is the **largest eigenvalue** of $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[1 3.7];[2 5.4];[3 5.6];[3.2 7.0]; [4 7.6];[5 7.9];[6 9.5]];\n",
    "\n",
    "% mean for subtracting \n",
    "my = ...\n",
    "\n",
    "% covariance matrix\n",
    "cy=...\n",
    "\n",
    "% eigenvectors and -values of covariance matrix\n",
    "% the second eigenvector has the larger eigenvalue, so we take that\n",
    "[evecs,evals]=eig(cy)\n",
    "\n",
    "% scatter data\n",
    "scatter(y(:,1),y(:,2)); grid on; xlim([0 7]); ylim([3 10]); hold on\n",
    "\n",
    "% draw first principal component from mean of the data\n",
    "drawArrow([my(1) my(1)+evecs(1,2)],[my(2) my(2)+evecs(2,2)],[0 7],[3 10],{'LineWidth',2,'Color','b'});\n",
    "\n",
    "% plot all the projected points in the PCA basis\n",
    "for p=1:size(y,1)\n",
    "    % IMPORTANT: THIS PROJECTION IS ONLY ONE NUMBER!! \n",
    "    % We are only interested in where the point lies ON THE FIRST PCA COMPONENT\n",
    "    yPCA = evecs(:,2)'*(y(p,:)'-my')\n",
    "    plot(my(1)+yPCA*evecs(1,2),my(2)+yPCA*evecs(2,2),'k.')\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we can see the direction is the same as the line we fit. \n",
    "\n",
    "So, finding this principal component as the direction of maximum variance in the data is the same (in two dimensions) as finding the direction of the line.\n",
    "\n",
    "Incidentally, the fact that we recover the eigenvectors of the covariance matrix is of course also related to our example of the quadratic forms from above!!"
   ]
  }
 ],
 "metadata": {
  "filename": "linear_algebra.rst",
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.1"
  },
  "title": "Linear Algebra"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
