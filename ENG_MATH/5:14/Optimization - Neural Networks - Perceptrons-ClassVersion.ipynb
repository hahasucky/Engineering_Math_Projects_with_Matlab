{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%plot inline -w 600 -h 600 -r 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Optimization - Neural Networks - The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Neural networks are excellent examples of (often non-linear) function optimization. \n",
    "\n",
    "Even though real neurons look like this:\n",
    "\n",
    "![Real Neuron](https://static.turbosquid.com/Preview/2014/12/03__09_02_06/all.jpg5254a144-6d9d-4a4f-b9b0-6fcbba4f64bcOriginal.jpg)\n",
    "\n",
    "and have highly complicated biochemical processes that control their firing given input in the many dendrites, people have long thought about how to approximate their function mathematically.\n",
    "\n",
    "\n",
    "\n",
    "### The artificial neuron\n",
    "\n",
    "We will briefly review some \"proper\" Nobel-prize-winning equations in the later part of the course, when we talk about partial derivative equations - however, as you may imagine, a real neuron's complexity is rather daunting, so for simulations, simpler models would be good as a start. \n",
    "\n",
    "Here's an extremely simplified model - a so-called artificial neuron which simply \n",
    "1. takes several inputs $x_i$\n",
    "2. sums them up as $\\sum x_i$\n",
    "3. pushes them through an activation function $f(\\sum x_i)$ and \n",
    "4. delivers one output $y=f(\\sum x_i)$ to downstream neurons.\n",
    "\n",
    "![Simplified neuron and model](https://cdn-images-1.medium.com/max/1200/1*SJPacPhP4KDEB1AdhOFy_Q.png)\n",
    "\n",
    "This model is based on the ground-breaking modeling work outlined in the paper by McCullough and Pitts from 1943. In this work, they used a step-function as $f$. \n",
    "\n",
    "McCulloch, W. and Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5:115â€“133.\n",
    "\n",
    "### Activation functions \n",
    "\n",
    "The purpose of the activation function is to introduce more complicated processing of the inputs - we will talk about this more later. The most common choices are plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file activation.m\n",
    "\n",
    "function out = activation(x,type)\n",
    "    if nargin==1\n",
    "        type='tanh';\n",
    "    end\n",
    "    \n",
    "    switch type\n",
    "        case 'tanh'\n",
    "            out = tanh(x);\n",
    "        case 'logistic'\n",
    "            out = 1./(1+exp(-x));\n",
    "        case 'relu'\n",
    "            out = max(zeros(size(x)),x);\n",
    "        case 'perceptron'\n",
    "            out = double(x>=0);\n",
    "        case 'linear'\n",
    "            out = x;\n",
    "        otherwise\n",
    "            error(['do not know type ' type])\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% x-values\n",
    "x=[-2:.01:2];\n",
    "\n",
    "% activation functions\n",
    "figure(); hold on\n",
    "plot(x,activation(x,'linear'),'displayname','linear');\n",
    "plot(x,activation(x,'tanh'),'displayname','tanh');\n",
    "plot(x,activation(x,'logistic'),'displayname','logistic');\n",
    "plot(x,activation(x,'perceptron'),'displayname','perceptron');\n",
    "plot(x,activation(x,'relu'),'displayname','relu');\n",
    "grid on\n",
    "ax = gca;\n",
    "ax.XAxisLocation = 'origin';\n",
    "ax.YAxisLocation = 'origin';\n",
    "legend('location','se')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning networks - the perceptron\n",
    "\n",
    "Let's try to do something simple now. Let's assume that our data we would like to learn consists of pairs $\\vec{x}_i,y_i$, and that the $y_i$ can only take on **two values** ($y_i=-1,1$). \n",
    "\n",
    "Furthermore, we assume that the data has a **linear structure**. We can quantify this like so: \n",
    "\n",
    "There exists some $\\vec{w}^* \\in \\mathbb{R}^d$, such that $\\|\\vec{w}^*\\|=1$  and for\n",
    "some $\\alpha > 0$, for all $i$ $\\in {1, 2, \\dots , n}$ it holds that:\n",
    "\n",
    "$$\n",
    "y_i(\\vec{w}^*\\vec{x}_i)>\\alpha\n",
    "$$\n",
    "\n",
    "Which, when you remember that $y_i=-1,1$, simply says that the sign of $(\\vec{w}^*\\vec{x}_i)$ is the same as the corresponding $y_i$.\n",
    "\n",
    "The constant $\\alpha$ is introduced as a lower bound on the value of $y_i(\\vec{w}^*\\vec{x}_i)$. \n",
    "\n",
    "Note, that if you find such a vector $\\vec{w}^*$, then all points that satisify\n",
    "\n",
    "$$\n",
    "(\\vec{w}^*\\vec{x})=0\n",
    "$$\n",
    "\n",
    "define a line (through the origin - if we augment our x-s with a leading \"1\" and add another leading $w_0$ to $\\vec{w}$, then we can also model the intercept or bias of that line)!\n",
    "\n",
    "\n",
    "Finally, we will assume that all input values $\\vec{x}_i$ are bounded from above (so the maximum distance from the origin for these points is some number $D$, such that for all $i$, $\\|\\vec{x}_i\\|<D$.\n",
    "\n",
    "How do we update the weights now? Let's write the algorithm created by Frank Rosenblatt in pseudo-code:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "set weights $\\vec{w}= $\n",
    "\n",
    "while any :\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;choose one index $k$, for which \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;set weights to \n",
    "\n",
    "</div>\n",
    "\n",
    "Let's write down the algorithm in Matlab - this is much longer than the pseudocode above, but only because of the plotting of the updated decision hyperplane that happens during the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file myPerceptron.m\n",
    "\n",
    "function w = myPerceptron(x,y,maxIter,doShuffle,doPlot)\n",
    "\n",
    "% init weights\n",
    "w = zeros(size(x,2),1);\n",
    "\n",
    "% check outputs\n",
    "outputs = ...\n",
    "\n",
    "iter=1;\n",
    "\n",
    "% plot data and initial guess\n",
    "xs=[min(x(:,2)):max(x(:,2))];\n",
    "if (doPlot)\n",
    "    indPos = y==1;\n",
    "    indNeg = y==-1;\n",
    "    scatter(x(indPos,2),x(indPos,3),'gx');\n",
    "    hold on; scatter(x(indNeg,2),x(indNeg,3),'ro');\n",
    "    % you should think where this comes from!\n",
    "    ys=(-w(2)*xs-w(1))./w(3);\n",
    "    plot(xs,ys,'b-')\n",
    "    xlim([xs(1) xs(end)]);\n",
    "    ylim([min(x(:,3)) max(x(:,3))]);\n",
    "    title(sprintf('%d: %d wrong\\n',iter,length(find(outputs))))\n",
    "    grid on;\n",
    "    \n",
    "end\n",
    "\n",
    "% as long as there any misclassified points\n",
    "% and we are within iteration limits, do:\n",
    "while(any(outputs) & iter<=maxIter)\n",
    "    % get all misclassified points\n",
    "    ind=...;\n",
    "    if (doPlot)\n",
    "        ys=(-w(2)*xs-w(1))./w(3);\n",
    "        plot(xs,ys,'b-')\n",
    "        xlim([xs(1) xs(end)]);\n",
    "        ylim([min(x(:,3)) max(x(:,3))]);\n",
    "        title(sprintf('%d: %d wrong\\n',iter,length(find(outputs))))\n",
    "        waitforbuttonpress();\n",
    "    end\n",
    "    % shuffle indices of points if desired\n",
    "    if (doShuffle)\n",
    "        ind = ind(randperm(length(ind)));\n",
    "    end\n",
    "    % update the weight with one misclassified point\n",
    "    w = ...;\n",
    "    iter=iter+1;\n",
    "    % and determine the new classification output\n",
    "    outputs = ...;\n",
    "end\n",
    "\n",
    "% plot the final solution\n",
    "if (doPlot)\n",
    "    ys=(-w(2)*xs-w(1))./w(3);\n",
    "    plot(xs,ys,'r-')\n",
    "    title(sprintf('%d: %d wrong\\n',iter,length(find(outputs))));\n",
    "    xlim([xs(1) xs(end)]);\n",
    "    ylim([min(x(:,3)) max(x(:,3))]);\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this algorithm with two reasonably well-separated point clouds in two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%plot native\n",
    "x = [randn(20,2);randn(20,2)+4];\n",
    "y = [-1*ones(20,1);ones(20,1)];\n",
    "w = myPerceptron([ones(length(x),1) x],y,50,true,true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see how the algorithm adjusts the line so that it tries to better capture the distribution of the data.\n",
    "\n",
    "The perceptron algorithm like this has two important properties:\n",
    "\n",
    "<div class=\"alert: alert-warning\">\n",
    "<p> 1. It ... found one line that can separate the classes.\n",
    "\n",
    "<p>2. It is an ... method as the weights are updated on ... point only!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%plot native\n",
    "x = [randn(20,2);randn(20,2)+2];\n",
    "y = [-1*ones(20,1);ones(20,1)];\n",
    "w = myPerceptron([ones(length(x),1) x],y,50,true,true);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can see that the algorithm (by definition) will not stop as it is not possible to find a line that splits the data.\n",
    "\n",
    "In this case, you will have to settle for the last decision-plane that the algorithm finds!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrons for Digit Recognition\n",
    "\n",
    "Let's apply the perceptrons to a \"real-world\" problem. We are going to use a digit dataset of 10000 digits from 0 to 9 that is shipped with the Matlab Deep Learning Toolbox.\n",
    "\n",
    "Since our perceptrons for now can only handle two classes, let's arbitrarily decide to learn to recognize \"0\" and \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file testPerceptronDigits.m\n",
    "\n",
    "function [wrongPercTrain,wrongPercTest]=testPerceptronDigits(numReps,numIters)\n",
    "\n",
    "% this prepares an imageDatastore for\n",
    "% the digit dataset shipped with Matlab\n",
    "digitDatasetPath = fullfile(matlabroot,'toolbox','nnet', ...\n",
    "    'nndemos','nndatasets','DigitDataset');\n",
    "imds = imageDatastore(digitDatasetPath, ...\n",
    "    'IncludeSubfolders',true, ...\n",
    "    'LabelSource','foldernames');\n",
    "% the dataset originally has numbers 0-9,\n",
    "% but we only want 0 and 1, so split\n",
    "imds=imds.partition(5,1);\n",
    "\n",
    "% check the labels of the data\n",
    "imds.countEachLabel\n",
    "\n",
    "% repeats the classification reps times\n",
    "% store performance\n",
    "wrongPercTrain=[];\n",
    "wrongPercTest=[];\n",
    "for rep = 1:numReps\n",
    "    % split the data into a training and a test set\n",
    "    [imdsTrain,imdsTest] = splitEachLabel(imds,0.95,'randomized');\n",
    "    \n",
    "    % read all images from train set into a cell array\n",
    "    Xim = imdsTrain.readall;\n",
    "    % show the pictures\n",
    "    if rep==1\n",
    "        montage(Xim)\n",
    "    end\n",
    "    % convert cell array into matrix [add bias term]\n",
    "    X = zeros(...);\n",
    "    % class labels\n",
    "    y = zeros(...);\n",
    "    for im = 1:length(X)\n",
    "        % data with bias term\n",
    "        X(im,:)=[...];\n",
    "        % first half is one class,\n",
    "        % second half is the other\n",
    "        if (im<=length(X)/2)\n",
    "            y(im)=...;\n",
    "        else\n",
    "            y(im)=...;\n",
    "        end\n",
    "    end\n",
    "    % train the perceptron\n",
    "    w = myPerceptron(X,y,numIters,true,false);\n",
    "    \n",
    "    % find misclassified images and display (if any)\n",
    "    wrong = ...;\n",
    "    wrongPercTrain(rep) = length(wrong)*100/size(X,1);\n",
    "    fprintf('rep %d: misclassifying %d digits = %.3f percent\\n',...\n",
    "        rep,length(wrong),wrongPercTrain(rep));\n",
    "    if ~isempty(wrong) & rep==1\n",
    "        figure(101);\n",
    "        montage(Xim(wrong))\n",
    "    end\n",
    "    \n",
    "    % show weights\n",
    "    if rep==1\n",
    "        figure(102);\n",
    "        imagesc(reshape(w(1:end-1),28,28))\n",
    "    end\n",
    "    \n",
    "    % now convert the TEST set of unseen images\n",
    "    Xim = imdsTest.readall;\n",
    "    Xtest = zeros(length(Xim),numel(Xim{1})+1);\n",
    "    ytest = zeros(length(Xim),1);\n",
    "    for im = 1:length(Xim)\n",
    "        Xtest(im,:)=[...];\n",
    "        if (im<=length(Xim)/2)\n",
    "            ytest(im)=...;\n",
    "        else\n",
    "            ytest(im)=...;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    % find misclassified images in TEST set\n",
    "    % and display if any\n",
    "    wrong = ...;\n",
    "    wrongPercTest(rep)=length(wrong)*100/size(Xtest,1);\n",
    "    fprintf('misclassifying %d test digits = %.3f percent\\n',...\n",
    "        length(wrong),wrongPercTest(rep));\n",
    "    if ~isempty(wrong) & rep==1\n",
    "        figure(103);\n",
    "        montage(Xim(wrong))\n",
    "    end\n",
    "end\n",
    "\n",
    "fprintf('\\n\\nTotal performance training = %.3f, testing = %.3f\\n',mean(wrongPercTrain),mean(wrongPercTest));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPerceptronDigits(1,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after 10 iterations, performance of the perceptron for both training and testing data is already above chance (which would be 50%).\n",
    "\n",
    "Let's try more iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPerceptronDigits(1,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that this works better - we now should have a lot lower training error - also the testing error should go down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPerceptronDigits(1,2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 2000 iterations, I've managed to get the training error down to 0, so the perceptron has fully learned how to separate the patterns of digits.\n",
    "\n",
    "The visualization of the weights is also interesting now, as these encode the orthogonal direction of the decision plane in the 784-dimensional space along which the patterns of 0s and 1s can be separated!\n",
    "\n",
    "There are usually still a few errors on the test set though, so let's do this across multiple splits of the train and test sets (which are randomly done, each time the function gets called)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPerceptronDigits(40,2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing 40 iterations, we can see that the test error consistently is higher than the training error. Overall, depending on the randomizations that are selected, we get around 2% testing error, whereas the training error is always 0%.\n",
    "\n",
    "This is a fundamental, incredibly important insight that holds for everything in learning and that is related to the function fitting discussion from earlier lectures:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p>0% on the training dataset does not mean ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of convergence for perceptrons\n",
    "\n",
    "How can we prove that this algorithm does its job? \n",
    "\n",
    "We can see that if it finishes, the line will separate the classes. So, what we need to prove is that the algorithm converges in a limited amount of steps $k$.\n",
    "\n",
    "Let's try to find a lower bound for $k$ first. Let's take a misclassified point $i$ and update it:\n",
    "\n",
    "$$\n",
    "\\vec{w}_{k+1}=\\vec{w}_k+y_i\\vec{x}_i\n",
    "$$\n",
    "\n",
    "we can multiply this by $\\vec{w}^*$ to get\n",
    "\n",
    "$$\n",
    "\\vec{w}_{k+1}\\vec{w}^*=\\vec{w}_k\\vec{w}^*+y_i\\vec{x}_i\\vec{w}^*\n",
    "$$\n",
    "\n",
    "but we required that the second term on the right - if we find a solution - is $>\\alpha$, so:\n",
    "\n",
    "$$\n",
    "\\vec{w}_{k+1}\\vec{w}^*>\\vec{w}_k\\vec{w}^*+\\alpha\n",
    "$$\n",
    "\n",
    "Now, let's start the process with $\\vec{w}_0=\\vec{0}$, which is our initialization of the weights. This means the update becomes:\n",
    "\n",
    "$$\n",
    "\\vec{w}_{1}\\vec{w}^*>\\alpha\n",
    "$$\n",
    "\n",
    "So, if we just did this $k$ times we therefore get:\n",
    "\n",
    "$$\n",
    "\\vec{w}_{k+1}\\vec{w}^*>k\\alpha\n",
    "$$\n",
    "\n",
    "And since $\\|\\vec{w}_{k+1}\\|\\|\\vec{w}^*\\|>\\vec{w}_{k+1}\\vec{w}^*$, we get:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{k+1}\\|>k\\alpha\n",
    "$$\n",
    "\n",
    "For the upper bound, let's write the norm of the update step:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{k+1}\\|^2=\\|\\vec{w}_k+y_i\\vec{x}_i\\|^2=\\|\\vec{w}_k\\|^2+2y_i\\vec{w}_k\\vec{x}_i+\\|\\vec{x}_i\\|^2<\\|\\vec{w}_k\\|^2+\\|\\vec{x}_i\\|^2\n",
    "$$\n",
    "\n",
    "where we've used the fact that the $|y_i|=1$. But we required bounded points: $\\|\\vec{x}_i\\|<D$, so:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{k+1}\\|^2<\\|\\vec{w}_k\\|^2+D^2\n",
    "$$\n",
    "\n",
    "Again, we start with $\\vec{w}_0=\\vec{0}$, so we get:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{1}\\|^2<D^2\n",
    "$$\n",
    "\n",
    "and doing that $k$ times, we get:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{k+1}\\|^2<kD^2\n",
    "$$\n",
    "\n",
    "Now, we've got two results - an upper and a lower bound:\n",
    "\n",
    "$$\n",
    "...\n",
    "$$\n",
    "\n",
    "so we \"ignore\" the middle term and get:\n",
    "\n",
    "$$\n",
    "...\n",
    "$$\n",
    "\n",
    "What that means is that the algorithm is **guaranteed** to ... in a maximum number of steps, provided the data is ... (related to the constant $\\alpha$) and that it is ... (related to the constant $D$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical functions with a perceptron\n",
    "\n",
    "The original paper by McCulloch and Pitts talked about using the neuronal model as a substitute for **logical** operations. So, let's try to following their reasoning.\n",
    "\n",
    "Let's say I want to build a neuron that receives two inputs $x_1,x_2$ - these inputs are logical values, true or false. I want the neuron to run a simple logical operation, so that, for example, its output $y$ will be equal to a target function, such as $t = x_1 \\text{ AND } x_2$. \n",
    "\n",
    "We can see that $t$ is a function with exactly two possible values as well. Given that the original perceptron was formulated as a two-class learner, this means we can try to apply this here as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputA=[0;0;1;1];\n",
    "inputB=[0;1;0;1];\n",
    "\n",
    "targetLogical=...;\n",
    "target=double(targetLogical);\n",
    "target(targetLogical==0)=-1;\n",
    "\n",
    "myPerceptron([ones(4,1) inputA inputB],target,20,true,true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So AND works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetLogical=...;\n",
    "target=double(targetLogical);\n",
    "target(targetLogical==0)=-1;\n",
    "\n",
    "myPerceptron([ones(4,1) inputA inputB],target,20,true,true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So OR works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetLogical=...);\n",
    "target=double(targetLogical);\n",
    "target(targetLogical==0)=-1;\n",
    "\n",
    "myPerceptron([ones(4,1) inputA inputB],target,20,true,true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, XOR does not work. We can see why since it cannot be linearly separated. \n",
    "\n",
    "The fact that such a simple logical function could not be processed by the perceptron was known for a long time. \n",
    "\n",
    "However, given that you can get all possible values of two logical inputs with a suitable **chain** of AND and NOT (or with OR and NOT) operations, you can see that implementing this function would be possible if you simply string enough perceptrons together!\n",
    "\n",
    "1969 saw the publication of a book \"Perceptrons\" by Marvin Minsky and Seymour Papert. Often it is said that the XOR problem was popularized by this book, which attributed to a dramatic decline in the popularity of neural networks - the first so-called AI winter. This is, however, only partly true since the result for XOR only holds for ONE SINGLE perceptron (again, a network of perceptrons would be fully capable of doing an XOR operation) - what Minsky and Papert showed, instead, is some limitations about those networks of perceptrons that related mostly to their EFFICIENCY, and not to the fact that they cannot in principle compute something. \n",
    "\n",
    "Regardless, the first AI winter did happen and funding for neural network based research did decline dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptrons\n",
    "\n",
    "It was already mentioned above that it should be possible to put multiple perceptron units together to create an actual neural **network**.\n",
    "\n",
    "We will talk about this in the next lecture, when we derive a more general way to train neurons and neural networks.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "filename": "linear_algebra.rst",
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.2"
  },
  "title": "Linear Algebra"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
