{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%plot inline -w 600 -h 600 -r 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Non-linear Optimization - Gradient descent \n",
    "\n",
    "## Nonlinear optimization\n",
    "\n",
    "Let's say you are at the top of a mountain and you want to get down to the sea. How do you do this?\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Choose a ...\n",
    "\n",
    "2. Go ....\n",
    "\n",
    "   2a. In which direction? \n",
    "\n",
    "   How about the direction that promises do get you ...? This of course is the ....\n",
    "\n",
    "   2b. How far do you walk?\n",
    "\n",
    "   Well that's a tricky question - if you make a huge, 1000km step in the direction of the gradient, then you may miss the sea. In contrast, if your step size is 1mm, then you will never arrive - the step size is a very important parameter of this algorithm\n",
    "   \n",
    "3. Stop if you've reached your minimum (the sea), i.e., if the gradient is zero.\n",
    "\n",
    "### Algorithm implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file gradientDescent.m\n",
    "\n",
    "function [minimum,xes] = gradientDescent(x0,g,stepSize,maxIter)\n",
    "% check input arguments\n",
    "switch nargin\n",
    "    case 2\n",
    "        stepSize = 0.01;\n",
    "        maxIter = 1000;\n",
    "    case 3\n",
    "        maxIter=1000;\n",
    "    case 1\n",
    "        error('call as gradientDescent(x0,g,stepSize,maxIter)')\n",
    "end\n",
    "\n",
    "% iteration number\n",
    "iter = 1;\n",
    "\n",
    "% init minimum candidate\n",
    "minimum = ...;\n",
    "\n",
    "% check whether we want to deliver history\n",
    "if nargout==2\n",
    "    xes = zeros(maxIter,1);\n",
    "end\n",
    "\n",
    "% as long as it's good\n",
    "while iter<=maxIter\n",
    "    % update history\n",
    "    if nargout==2\n",
    "        xes(iter)=minimum;\n",
    "    end\n",
    "    \n",
    "    % do one update step of gradient descent\n",
    "    minimum = ...;\n",
    "    ...;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm tested\n",
    "\n",
    "Let's test our algorithm with a simple function $f(x)=\\cos(x)*e^{2.5x}$ and a starting point of $x=0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% define function to minimize\n",
    "f = @(x) cos(x+0.5).*exp(0.5*x);\n",
    "% its gradient\n",
    "g = @(x) ...;\n",
    "\n",
    "% starting point\n",
    "x0 = 1.01;\n",
    "% plot both f and g\n",
    "fplot(f); hold on\n",
    "fplot(g)\n",
    "% do the gradient descent and get history as well\n",
    "[minimum,xes] = gradientDescent(x0,g,.1,800);\n",
    "fprintf('found minimum @x=%f\\n',minimum);\n",
    "% plot history and found minimum into plot\n",
    "plot(xes,f(xes),'kx')\n",
    "plot(minimum,f(minimum),'ro')\n",
    "\n",
    "grid on;\n",
    "ax = gca;\n",
    "ax.XAxisLocation = 'origin';\n",
    "ax.YAxisLocation = 'origin';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about making the step size larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% define function to minimize\n",
    "f = @(x) cos(x+0.5).*exp(0.5*x);\n",
    "% its gradient\n",
    "g = @(x) ...;\n",
    "\n",
    "% starting point\n",
    "x0 = 1.01;\n",
    "% plot both f and g\n",
    "fplot(f); hold on\n",
    "fplot(g)\n",
    "% do the gradient descent and get history as well\n",
    "[minimum,xes] = gradientDescent(x0,g,1,800);\n",
    "fprintf('found minimum @x=%f\\n',minimum);\n",
    "% plot history and found minimum into plot\n",
    "plot(xes,f(xes),'kx')\n",
    "plot(minimum,f(minimum),'ro')\n",
    "\n",
    "grid on;\n",
    "ax = gca;\n",
    "ax.XAxisLocation = 'origin';\n",
    "ax.YAxisLocation = 'origin';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops. That is a minimum, but that's one on the other side. This illustrates what happens if you go too far into the direction of the gradient - here the gradient first went down, then up and then to the other side of the function!\n",
    "\n",
    "This is a big problem with gradient descent!\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p>Gradient descent needs a ...\n",
    "</div>\n",
    "\n",
    "How about a different starting point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% define function to minimize\n",
    "f = @(x) cos(x+0.5).*exp(0.5*x);\n",
    "% its gradient\n",
    "g = @(x) ...;\n",
    "\n",
    "% starting point\n",
    "x0 = -.5;\n",
    "% plot both f and g\n",
    "fplot(f); hold on\n",
    "fplot(g)\n",
    "% do the gradient descent and get history as well\n",
    "[minimum,xes] = gradientDescent(x0,g,.1,800);\n",
    "fprintf('found minimum @x=%f\\n',minimum);\n",
    "% plot history and found minimum into plot\n",
    "plot(xes,f(xes),'kx')\n",
    "plot(minimum,f(minimum),'ro')\n",
    "\n",
    "grid on;\n",
    "ax = gca;\n",
    "ax.XAxisLocation = 'origin';\n",
    "ax.YAxisLocation = 'origin';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we had a small step-size, but we slid into the other side of the function as well - the gradient pointed that way!!\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p>Even with such a suitably small step-size, gradient descent only is guaranteed to find <b>...</b>!!\n",
    "</div>\n",
    "<p><p>\n",
    "\n",
    "\n",
    "### Upgrading gradient descent\n",
    "\n",
    "You can of course do **much better** than the simple implementation given above. \n",
    "\n",
    "For example:\n",
    "\n",
    "1. You can stop if either the gradient is small enough, or if the changes between two consecutive $x$ is small enough\n",
    "\n",
    "2. You could adapt the step-size to the gradient of the function - if it's small, you can step further than when it is large!\n",
    "\n",
    "3. You can implement ... in order to perhaps have a chance to find your true, global minimum.\n",
    "\n",
    "This is not doing better, but you can of course also use the numeric gradient descent if your function is a tad too complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% define rather complicated f\n",
    "f = @(x) cos(x+0.5).*exp(0.5*sin(x)+log(x.^2));\n",
    "% I had some help here from Mathworks ^^\n",
    "realg = @(x) cos(x + 1/2).*exp(log(x.^2) + sin(x)/2).*(cos(x)/2 + 2./x) - sin(x + 1/2).*exp(log(x.^2) + sin(x)/2);\n",
    "% or I can just be lazy!\n",
    "g = @(x) ...;\n",
    "\n",
    "% plot both f and g/realg\n",
    "fplot(f); hold on\n",
    "fplot(g); fplot(realg) % you should not see a difference!\n",
    "\n",
    "% starting points!\n",
    "x0 = [-4.9:.5:5];\n",
    "% time it and do gradient descent with the numerical gradient\n",
    "tic\n",
    "minimum=[];\n",
    "for i=1:length(x0)\n",
    "    minimum(i) = gradientDescent(x0(i),g,.01,800);\n",
    "    fprintf('%d: found minimum @x=%f\\n',i,minimum(i));\n",
    "end\n",
    "toc\n",
    "% time it and do gradient descent with the real gradient\n",
    "tic\n",
    "minimum=[];\n",
    "for i=1:length(x0)\n",
    "    minimum(i) = gradientDescent(x0(i),realg,.01,800);\n",
    "    fprintf('%d: found minimum @x=%f\\n',i,minimum(i));\n",
    "end\n",
    "toc\n",
    "\n",
    "% plot all minima\n",
    "plot(minimum,f(minimum),'ro')\n",
    "% find the lowest of all minima!\n",
    "...;\n",
    "plot(minimum(minind),fmin,'ro','LineWidth',2)\n",
    "\n",
    "grid on;\n",
    "ax = gca;\n",
    "ax.XAxisLocation = 'origin';\n",
    "ax.YAxisLocation = 'origin';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the two gradients seem pretty close.\n",
    "\n",
    "Gradient descent seems to be a tad shorter for the real gradient - possibly due to the fact that function calling has an overhead and we do that twice in g. \n",
    "\n",
    "In addition, there are slight numerical discrepancies between the two methods - these could be due to rounding errors in either g or realg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-fitting using gradient descent\n",
    "\n",
    "Let's do our vanilla demo example of fitting a line to data. \n",
    "\n",
    "Our error function for fitting a line to data is:\n",
    "\n",
    "$$\n",
    "f(w,b)=\\sum_i \\left ( (wx_i+b) - y_i\\right )^2\n",
    "$$\n",
    "\n",
    "How can we write this more compactly? We remember linear algebra and the Vandermonde matrix!\n",
    "\n",
    "If we make a new matrix $X$ with\n",
    "$$\n",
    "X:=\\left ( \\begin{matrix}1&x_1\\\\1&x_2\\\\\\vdots&\\vdots\\\\1&x_n\\end{matrix} \\right )\n",
    "$$\n",
    "\n",
    "and combine our parameters $w,b$ into one vector $\\vec{p}=(w,b)^{\\top}$, then our problem from above becomes:\n",
    "\n",
    "$$\n",
    "f(\\vec{p})=...\n",
    "$$\n",
    "\n",
    "To make it easier, we add a factor of 0.5 to $E$ and write it out as an inner product:\n",
    "\n",
    "$$\n",
    "f(\\vec{p})=...\n",
    "$$\n",
    "\n",
    "which we can easily derive with respect to $\\vec{p}$ to get its derivative!\n",
    "\n",
    "So let's fit a line to our linear data using gradient descent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% get our line data\n",
    "data = [[1 3.7];[2 5.4];[3 5.6];[3.2 7.0]; [4 7.6];[5 7.9];[6 9.5]];\n",
    "% get x and y coordinates separately\n",
    "x = data(:,1); y = data(:,2);\n",
    "% create Vandermonde matrix for polynomial of first degree\n",
    "X = [x repmat(1,length(data),1)]; \n",
    "% this is the error function for fitting the polynomial then\n",
    "f  = @(p) ...;\n",
    "% and this is its gradient - note that this has n+1 entries!\n",
    "gw = @(p) ...;\n",
    "\n",
    "% you want to be lazy? I have you covered!\n",
    "%gw = @(p) cat(2,(f([p(1)+0.0001,p(2)])-f(p))/.0001,(f([p(1),p(2)+0.0001])-f(p))/.0001);;\n",
    "\n",
    "% starting point for w and b\n",
    "p0=[-.5;.5];\n",
    "% define small step-size\n",
    "stepSize=.00005;\n",
    "% iteration number\n",
    "iter=1;\n",
    "% init minimum candidate\n",
    "minimum = p0;\n",
    "% plot the data and the error function\n",
    "figure(101)\n",
    "scatter(x,y); xlabel('x');ylabel('y');hold on;\n",
    "figure(102);\n",
    "% create suitable combinations of w and b to plot\n",
    "[ws,bs]=meshgrid([-1:.1:2],[.4:.1:3]);\n",
    "% make a plot of the error function by looping through w,b\n",
    "fs=[];\n",
    "for r=1:size(ws,1)\n",
    "    for c=1:size(ws,2)\n",
    "        fs(r,c)=f([ws(r,c);bs(r,c)]);\n",
    "    end\n",
    "end\n",
    "surf(ws,bs,fs); view(45,20); xlabel('w');ylabel('b');zlabel('f(w,b)');hold on\n",
    "\n",
    "% do the gradient descent\n",
    "while iter<=20000\n",
    "    % update plots from time-to-time\n",
    "    if (mod(iter-1,100)==0)\n",
    "        figure(101)\n",
    "        plot(x,minimum(1)*x+minimum(2),'b-','LineWidth',.5)\n",
    "        figure(102)\n",
    "        plot3(minimum(1),minimum(2),f(minimum),'ro')\n",
    "    end\n",
    "    % do one update step\n",
    "    minimum = ...;\n",
    "    iter=iter+1;\n",
    "end\n",
    "fprintf('minimum = %f,%f',minimum(1),minimum(2))\n",
    "\n",
    "figure(101)\n",
    "plot(x,minimum(1)*x+minimum(2),'r-','linewidth',2)\n",
    "grid on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these two plots, we can learn a lot of things about gradient descent. Perhaps the most interesting plot is the lower one, which plots the error surface and the values of the error function as gradient descent walks down.\n",
    "\n",
    "We can see a first large step in the direction of $w$ - this clearly has the most influence of the two parameters in terms of determining the direction. This is done a few more times, and then the line basically has almost the correct slope $w$. Next, the system walks down the more shallow slope of the parameter $b$, which is then optimized in many more smaller steps.\n",
    "\n",
    "Hence we learn that optimizing the intercept $b$ here is much \"harder\" or \"slower\" than optimizing its slope using gradient descent! \n",
    "\n",
    "\n",
    "This issue is also connected to the issue of scaling of the features - if one of the input dimensions has a vastly different scaling, gradient descent will be sensitive to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Let's say you want to fit a line, but you have a million points.\n",
    "\n",
    "Looking at our code, this means that we need to manipulate and multiply a Vandermonde matrix with a million rows several times.\n",
    "\n",
    "This is technically do-able, but even modern computers with lots of memory will run out of memory like this.\n",
    "\n",
    "How do we overcome this issue? We have a crucial idea:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p>If we assume that our million data points all carry some sort of information about our problem, we can exploit the redundancy in that dataset by simply evaluating the gradient on a ....\n",
    "</div>\n",
    "\n",
    "This method (and its many variations) is called stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totPoints = 100000;\n",
    "% form a line with random noise added\n",
    "x = linspace(0,100,totPoints)'; y = x*1.1+100.0+10*randn(totPoints,1);\n",
    "% since there may be many points, plot only at most 1000\n",
    "scatterInd = randperm(totPoints);\n",
    "scatterInd = scatterInd(1:min(totPoints,1000));\n",
    "figure(201);scatter(x(scatterInd),y(scatterInd)); hold on\n",
    "% create Vandermonde matrix [note, this is not actually necessary!]\n",
    "X = [x repmat(1,totPoints,1)]; \n",
    "% error function - very expensive to compute with lots of samples!\n",
    "f  = @(p) ...;\n",
    "% stochastic gradient evaluated at certain indices i\n",
    "gwi = @(p,i) ...;\n",
    "% store function values for later plotting\n",
    "fs=[];\n",
    "% do this whole process 10 times\n",
    "for sample=1:10\n",
    "    % starting point\n",
    "    minimum=[-.5;20];\n",
    "    % iteration number\n",
    "    iter=1;\n",
    "    % step size for gradient descent\n",
    "    stepSize0=.0001;\n",
    "    % give it time to at least see all the points once or twice\n",
    "    while iter<=2*totPoints\n",
    "        % update plots\n",
    "        if (mod(iter-1,1000)==0)\n",
    "            fs(sample,ceil(iter/1000))=log(f(minimum));\n",
    "        end\n",
    "        % this selects ONE random index from all points!\n",
    "        i=1+floor(rand*totPoints);\n",
    "        % this would ensure that we walk slower and slower\n",
    "        % the parameter totPoints is the number of update steps\n",
    "        % that are used to stabilize the solution\n",
    "        % stepSize=stepSize0/(1+iter/totPoints);\n",
    "        % standard step-size\n",
    "        stepSize=stepSize0;\n",
    "        % update step\n",
    "        minimum = ...;\n",
    "        iter=iter+1;\n",
    "    end\n",
    "    fprintf('sample %d: minimum = %f,%f\\n',sample,minimum(1),minimum(2))\n",
    "    figure(201)\n",
    "    plot(x,minimum(1)*x+minimum(2),'r-','LineWidth',2)\n",
    "end\n",
    "% plots the current error function value - expensive!\n",
    "figure(202);plot(fs'); grid on; \n",
    "xlabel('Iteration (1000s)'); ylabel('Log(Error)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 10 repetitions (samples) yield similar, but slightly different solutions - similarly, the evolution of the error of our solution is different for the 10 repetitions. We can also clearly see the noise inherent in the stochastic gradient descent and the fact that this noise **grows** towards the end of the iterations!\n",
    "\n",
    "Note that this method of course only works if all points have some sort of relation to the model. In our case, we have no outliers, but simply some random Gaussian noise on our linear data, so that our model is reasonably well-behaved.\n",
    "\n",
    "In addition, the line fit only has two parameters, but have several thousand points, which makes the minimum \"more unique\". We will see changes in the shape of the error curves when we go to optimization with a much larger number of parameters in the next lectures on neural networks.\n",
    "\n",
    "### Upgrading stochastic gradient descent\n",
    "\n",
    "This is an art-form in and of itself. You can add the same changes to this as before (updating of the learning rate, etc.). This is already indicated in the code above - however, this introduces several other parameters, and how to do this properly is not at all intuitive. Any learning rate decay, however, will try to prevent overly large fluctuations of the gradient towards the end of the iterations.\n",
    "\n",
    "Another change that is worthwhile pursuing is to group the gradient calculation into small **mini-batches**, in which say $n=10$ data points are used to upgrade the gradient. We will talk about this briefly in the next lecture as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "filename": "linear_algebra.rst",
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.1"
  },
  "title": "Linear Algebra"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
