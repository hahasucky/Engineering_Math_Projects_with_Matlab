{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%plot inline -w 600 -h 600 -r 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Optimization - Neural Networks - More networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning in a better way - gradient descent!\n",
    "\n",
    "Let's return to our very simple model of neurons, for which we said that we can write their output as the activation function applied to the linearly weighted input:\n",
    "$$\n",
    "\\vec{y}=f(\\vec{w}^{\\top}\\vec{x})\n",
    "$$\n",
    "\n",
    "\n",
    "Now, we need to find a way to **learn** the weights $\\vec{w}$ somehow. \n",
    "\n",
    "In the previous perceptron examples, we did this by updating the weights with the class-weighted sample - $\\vec{w}=\\vec{w}+y_k\\vec{x}_k$, but how would we do this in a general case?\n",
    "\n",
    "Let's try to do **gradient descent**!!\n",
    "\n",
    "Let's again go to our standard way of doing things and minimize the least squares error of some target values $\\vec{t}$ versus the prediction:\n",
    "\n",
    "$$\n",
    "E(\\vec{w})=\\frac{1}{2} \\sum \\|\\vec{y(\\vec{w})}-\\vec{t}\\|^2\n",
    "$$\n",
    "\n",
    "Now we remember that the output is some kind of function of the weighted sum of all inputs $\\vec{x}$, i.e., $\\vec{y}=f(\\vec{w}^{\\top}\\vec{x})$, so we can take the derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(\\vec{w})}{\\partial \\vec{w}}= \\left ( f(\\vec{w}^{\\top}\\vec{x})-\\vec{y}\\right ) \\frac{\\partial f(\\vec{w}^{\\top}\\vec{x})}{\\partial \\vec{w}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\left ( f(\\vec{w}^{\\top}\\vec{x})-\\vec{y}\\right )\\vec{x} \\frac{\\partial f(\\vec{w}^{\\top}\\vec{x})}{\\partial(\\vec{w}^{\\top}\\vec{x})}\n",
    "$$\n",
    "\n",
    "So, once we've decided on an activation function, we can determine this derivative.\n",
    "\n",
    "#### $f=x$\n",
    "\n",
    "If the activation function $f$ is simply the identity function $f(\\vec{i}) = \\vec{i}$, then this derivative becomes very simple:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(\\vec{w})}{\\partial \\vec{w}}=\\left ( \\vec{w}^{\\top}\\vec{x}-\\vec{y}\\right )\\vec{x} \n",
    "$$\n",
    "\n",
    "So, now our update for the gradient can be calculated, and it will be:\n",
    "\n",
    "$$\n",
    "-\\lambda\\frac{\\partial E(\\vec{w})}{\\partial \\vec{w}}=-\\lambda \\left ( \\vec{w}^{\\top}\\vec{x}-\\vec{y}\\right )\\vec{x} \n",
    "$$\n",
    "\n",
    "#### $f=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "If the activation function $f$ is the logistic function $f=\\frac{1}{1+e^{-\\vec{w}^{\\top}\\vec{x}}}$, then this derivative becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(\\vec{w})}{\\partial \\vec{w}}=\\left ( f(\\vec{w}^{\\top}\\vec{x})-\\vec{y}\\right )\\vec{x}f(\\vec{w}^{\\top}\\vec{x})(1-f(\\vec{w}^{\\top}\\vec{x})) \n",
    "$$\n",
    "\n",
    "#### $f=\\text{ReLU}=\\max(0,x)$\n",
    "\n",
    "This function is not differentiable at $x=0$, so we will arbitrarily say that\n",
    "\n",
    "$$\n",
    "f'(x)=\\left \\{ \\begin{array}{cc}1&: x>0\\\\0&:x\\leq 0 \\end{array} \\right .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer networks: Backpropagation\n",
    "Now, this approach is all fine, if we only have ONE neuron, but what about multiple, connected neurons that live in different layers?\n",
    "\n",
    "The error in the output of an arbitrary neuron in a neural network will depend on all previous weights, so if we want to do gradient descent, we need to somehow know how to derive this.\n",
    "\n",
    "Again, we assume that each neuron $j$ will provide an output $y_j$, which is defined as the weighted sum of inputs pushed through the activation function:\n",
    "\n",
    "$$\n",
    "y_j = f(\\vec{w}\\vec{x})=f(\\sum w_{kj}x_k)\n",
    "$$\n",
    "\n",
    "So, the derivative of the total error with respect to any weight $w_{ij}$ is done using the full sets of dependents:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}}=\\frac{\\partial E}{\\partial y_{j}}\\frac{\\partial y_j}{\\partial w_{ij}}=\\frac{\\partial E}{\\partial y_{j}}\\frac{\\partial y_j}{\\partial (\\sum w_{kj}y_k)}\\frac{\\partial (\\sum w_{kj}y_k)}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "Let's look at the three terms. Let's start with the second term, which is the derivative of the error function. Let's assume it's the logistic function, which has the neat and simple derivative from above\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_j}{\\partial (\\sum w_{kj}y_k)}=...\n",
    "$$\n",
    "\n",
    "For the third term we observe that in the sum only one term contains the actual derivative weight $w_{ij}$, so that becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\sum w_{kj}y_k)}{\\partial w_{ij}}=...\n",
    "$$\n",
    "\n",
    "Now, for the first term, we distinguish two cases: if the neuron is in the final layer, then $y_j=y_{end}$, which means:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial y_{j}}=\\frac{\\partial E}{\\partial y_{end}}= \\frac{\\partial}{\\partial y_{end}}\\frac{1}{2}(t-y_{end})^2=...\n",
    "$$\n",
    "\n",
    "For \"inner\" neurons $j$ in other layers, however, this term needs to be evaluated with respect to earlier layers as well, since the error term depends on the outputs of all neurons connected to the present neuron. Let's put all these $a$ connected neurons into one set $N=\\{n_{1j},\\dots,n_{aj}\\}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(y_{j})}{\\partial y_{j}}=\n",
    "\\sum_{n\\in N}\\left ( \\frac{\\partial E}{\\partial (\\sum w_{nj}y_n)}\\frac{\\partial (\\sum w_{nj}y_n)}{\\partial y_j} \\right )=\\sum_{n\\in N}\\left ( \\frac{\\partial E}{\\partial y_n}\\frac{\\partial y_n}{\\partial (\\sum w_{nj}y_n)}\\frac{\\partial (\\sum w_{nj}y_n)}{\\partial y_j} \\right )=\\sum_{n\\in N}\\left ( \\frac{\\partial E}{\\partial y_n}\\frac{\\partial y_n}{\\partial (\\sum w_{nj}y_n)}w_{jn} \\right )\n",
    "$$\n",
    "\n",
    "This means that we can calculate the derivative, once we know all of the derivatives with respect to the downstream connected neurons are known. This is why this is called backpropagation, since in order to run the algorithm, you start from the output layer, and then proceed backwards through the network!\n",
    "\n",
    "In total, we get for a logistic activation function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}}=y_i\\alpha_j\n",
    "$$\n",
    "\n",
    "with \n",
    "$$\n",
    "\\alpha_j:=\\frac{\\partial y_j}{\\partial (\\sum w_{kj}y_k)}\\frac{\\partial (\\sum w_{kj}y_k)}{\\partial w_{ij}}=\\left \\{ \\begin{array}{ll}... &: \\text{if }j\\text{ is an output layer neuron}\\\\...&: \\text{if }j\\text{ is an inner/hidden layer neuron} \\end{array}\\right .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example implementation\n",
    "\n",
    "Here's an example implementation that learns a simple one hidden-layer network with a few neurons in order to approximate a $f(x,y)=x^2+y^2+1$.\n",
    "\n",
    "We generate 400 input/output value pairs and then try to learn the whole thing using our update rules from above.\n",
    "\n",
    "Just for fun, here we are going to use the ReLU function instead of the logistic function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file oneHiddenLayerRegressionNetwork.m\n",
    "\n",
    "function oneHiddenLayerRegressionNetwork(hidden_layer_size,numEpochs,lr)\n",
    "input_layer_size = 2;\n",
    "output_layer_size = 1;\n",
    "\n",
    "% init weights\n",
    "Wh = randn(input_layer_size, hidden_layer_size) * sqrt(2.0/input_layer_size);\n",
    "Wo = randn(hidden_layer_size, output_layer_size) * sqrt(2.0/hidden_layer_size);\n",
    "\n",
    "% init biases\n",
    "Bh = ones(1, hidden_layer_size)*0.1;\n",
    "Bo = ones(1, output_layer_size)*0.1;\n",
    "\n",
    "% create data for regression\n",
    "xs=linspace(-2,2,20);\n",
    "ys=linspace(-2,2,20);\n",
    "counter=1;\n",
    "data=[];\n",
    "labels=[];\n",
    "for r=1:length(xs)\n",
    "    for c=1:length(ys)\n",
    "        data(counter,:)=[xs(r) ys(c)];\n",
    "        labels(counter,1)=xs(r)^2+ys(c)^2+1;\n",
    "        counter=counter+1;\n",
    "    end\n",
    "end\n",
    "\n",
    "% normalize data\n",
    "data(:,1)=data(:,1)/norm(data(:,1));\n",
    "data(:,2)=data(:,2)/norm(data(:,2));\n",
    "\n",
    "numData = size(data,1);\n",
    "\n",
    "% learning rate\n",
    "lr = .001;\n",
    "\n",
    "\n",
    "%% main loop\n",
    "for iter=1:numEpochs\n",
    "    % this is stochastic gradient descent\n",
    "    for j = 1:numData\n",
    "        % select a random data point\n",
    "        selectedInd = 1+floor(rand*numData);\n",
    "        % set the current data\n",
    "        selectedData = data(selectedInd,:);\n",
    "        selectedLabel = labels(selectedInd);\n",
    "        % do one step of backpropagation on that data\n",
    "        [Wh,Wo]=backprop(selectedData, selectedLabel, Wh, Bh, Wo, Bo, lr);\n",
    "    end\n",
    "    % write out result\n",
    "    if mod(iter,100)==0\n",
    "        [yHat,Zo,Zh,H] = feed_forward(data, Wh, Bh, Wo, Bo);\n",
    "        fprintf('iter %05d: %f\\n',iter,loss(yHat,labels));\n",
    "    end\n",
    "end\n",
    "%% check final result\n",
    "[yHat,Zo,Zh,H] = feed_forward(data, Wh, Bh, Wo, Bo);\n",
    "fprintf('iter %05d: %f\\n',iter,loss(yHat,labels));\n",
    "[xsm,ysm]=meshgrid(xs,ys);\n",
    "figure(101);\n",
    "subplot(1,2,1)\n",
    "surf(xsm,ysm,xsm.^2+(ysm.^2)+1)\n",
    "subplot(1,2,2)\n",
    "surf(xsm,ysm,reshape(yHat,length(xs),length(xs)))\n",
    "end\n",
    "\n",
    "%% this is the relu activation function\n",
    "function r = relu(Z)\n",
    "    r = max(0, Z);\n",
    "end\n",
    "\n",
    "%% this is the feed_forward function\n",
    "function [yHat,Zo,Zh,H] = feed_forward(X,Wh,Bh,Wo,Bo)\n",
    "    %\n",
    "    %X    - input matrix\n",
    "    %Zh   - hidden layer weighted input\n",
    "    %Zo   - output layer weighted input\n",
    "    %H    - hidden layer activation\n",
    "    %y    - output layer\n",
    "    %yHat - output layer predictions\n",
    "    %\n",
    "\n",
    "    % Hidden layer function\n",
    "    Zh = X*Wh + repmat(Bh,size(X,1),1);\n",
    "    H = relu(Zh);\n",
    "\n",
    "    % Output layer function\n",
    "    Zo = H*Wo + repmat(Bo,size(X,1),1);\n",
    "    yHat = relu(Zo);\n",
    "end\n",
    "\n",
    "%% derivative of relu\n",
    "function o = relu_der(z)\n",
    "    o = double(z > 0);\n",
    "end\n",
    "\n",
    "%% calculate loss\n",
    "function l = loss(yHat, y)\n",
    "    l= 0.5 * (yHat - y)'*(yHat-y);\n",
    "end\n",
    "\n",
    "%% derivative of loss\n",
    "function ld = loss_der(yHat, y)\n",
    "    ld= yHat - y;\n",
    "end\n",
    "\n",
    "%% backpropagation step\n",
    "function [Wh,Wo]=backprop(data, labels, Wh, Bh, Wo, Bo, lr)\n",
    "    % push the data through the network\n",
    "    [yHat,Zo,Zh,H] = feed_forward(data, Wh, Bh, Wo, Bo);\n",
    "    \n",
    "    % layer errors for output layer\n",
    "    Eo = (yHat - labels).* double(Zo>0);\n",
    "    % layer error for input layer\n",
    "    Eh = Eo .* double(Zh>0) ;\n",
    "\n",
    "    % loss derivative for weight of output layer\n",
    "    dWo = Eo*H;\n",
    "    \n",
    "    % loss derivative for weight of hidden layer\n",
    "    dWh = Eh'* data;\n",
    "\n",
    "    % update weights\n",
    "    Wh = Wh-lr * dWh';\n",
    "    Wo = Wo-lr * dWo';\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "Let's try to get a feeling for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHiddenLayerRegressionNetwork(3,1000,.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHiddenLayerRegressionNetwork(10,1000,.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHiddenLayerRegressionNetwork(20,1000,.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHiddenLayerRegressionNetwork(20,5000,.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "filename": "linear_algebra.rst",
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.1"
  },
  "title": "Linear Algebra"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
